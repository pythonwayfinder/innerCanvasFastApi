# main.py

# pip install fastapi "uvicorn[standard]" python-multipart tensorflow numpy opencv-python Pillow transformers sentencepiece openai

import os
from dotenv import load_dotenv

# ğŸš€ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°€ì¥ ì²« ë‹¨ê³„ì—ì„œ .env íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# Hugging Face Tokenizerì˜ ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™” (macOS, Windows ì¶©ëŒ ë°©ì§€)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from pydantic import BaseModel
from typing import List, Optional, Any, Dict

# --- ê° ëª¨ë“ˆì—ì„œ ë¶„ì„ í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜´ ---
from CNN import analyze_doodle_cnn
from koBERT_model import analyze_emotion, initialize_model as initialize_kobert, DiaryRequest
from LLM_RAG import chat_with_rag, chat_with_rag_for_chat


# --- AI ëª¨ë¸ë“¤ì„ ì €ì¥í•  ë³€ìˆ˜ ---
models = {}

# --- FastAPI ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‘ì—… ì²˜ë¦¬ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    ì„œë²„ê°€ ì‹œì‘ë  ë•Œ AI ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë”©í•˜ì—¬ API ì‘ë‹µ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    print("ğŸš€ ì„œë²„ ì‹œì‘! AI ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    # KoBERT ëª¨ë¸ ë¡œë”©
    kobert_model, tokenizer, device, labels = initialize_kobert()
    models["kobert_model"] = kobert_model
    models["tokenizer"] = tokenizer
    models["device"] = device
    models["LABELS"] = labels
    print("âœ… [KoBERT] ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
    # CNN ëª¨ë¸ì€ cnn.pyë¥¼ importí•  ë•Œ ìë™ìœ¼ë¡œ ë¡œë”©ë©ë‹ˆë‹¤.
    # LLM ê´€ë ¨ ì„¤ì •ì€ llm_rag.pyë¥¼ importí•  ë•Œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    yield
    # --- ì„œë²„ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë  ì½”ë“œ ---
    print("ğŸŒ™ ì„œë²„ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")
    models.clear()

# --- FastAPI ì•± ìƒì„± ---
app = FastAPI(lifespan=lifespan)

@app.get("/")
def read_root():
    """ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ê¸°ë³¸ ê²½ë¡œ"""
    return {"status": "Inner Canvas AI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}

@app.post("/analyze/diary/")
async def analyze_diary_endpoint(
    text: str = Form(...),
    file: UploadFile = File(...)
):
    """ì‚¬ìš©ìì˜ ì¼ê¸°(í…ìŠ¤íŠ¸+ê·¸ë¦¼)ë¥¼ ë°›ì•„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìƒë‹´ ë‹µë³€ì„ ë°˜í™˜í•˜ëŠ” ë©”ì¸ API"""
    try:
        # 1. ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
        image_bytes = await file.read()

        # 2. CNNìœ¼ë¡œ ê·¸ë¦¼ ë¶„ì„
        cnn_result = analyze_doodle_cnn(image_bytes)
        if "error" in cnn_result:
            raise HTTPException(status_code=500, detail=cnn_result["error"])

        # 3. KoBERTë¡œ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ (ë¯¸ë¦¬ ë¡œë”©ëœ ëª¨ë¸ ì‚¬ìš©)
        kobert_request = DiaryRequest(text=text)
        kobert_result = analyze_emotion(
            request=kobert_request,
            model=models["kobert_model"],
            tokenizer=models["tokenizer"],
            device=models["device"],
            LABELS=models["LABELS"]
        )
        if "error" in kobert_result:
            raise HTTPException(status_code=500, detail=kobert_result["error"])
        
        # 4. LLM(RAG)ìœ¼ë¡œ ìµœì¢… ìƒë‹´ ë‹µë³€ ìƒì„±
        # llm_rag.pyì˜ í•¨ìˆ˜ í˜•ì‹ì— ë§ê²Œ kobert ê²°ê³¼ë¥¼ ì„ì‹œ ë³€í™˜
        temp_kobert_for_rag = {"sentiment": kobert_result.get("emotion_type"), "score": 1.0} 
        
        final_answer, _ = chat_with_rag(text, cnn_result, temp_kobert_for_rag)

        # 5. ìµœì¢… ê²°ê³¼ ì¢…í•© ë° ì‘ë‹µ
        final_response = {
            "counseling_response": final_answer,
            "analysis_details": {
                "doodle_prediction": cnn_result,
                "text_emotion": kobert_result,
            }
        }
        return JSONResponse(content=final_response)

    except Exception as e:
        print("DEBUG ERROR:", str(e))
        # ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬ ì²˜ë¦¬
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")

# ==========================================
# Request/Response ëª¨ë¸ ì •ì˜
# ==========================================
class ChatHistoryItem(BaseModel):
    chatId: Optional[int] = None
    sender: Optional[str] = None
    message: Optional[str] = None
    createdAt: Optional[str] = None


class ChatRequest(BaseModel):
    diaryId: Optional[int] = None
    message: Optional[str] = None
    currentChatHistory: Optional[List[ChatHistoryItem]] = None
    past7DaysHistory: Optional[str] = None  # JSON ë¬¸ìì—´ í˜•íƒœë¡œ ìŠ¤í”„ë§ì—ì„œ ì „ë‹¬ë¨


class ChatResponse(BaseModel):
    message: Optional[str] = None

# ==========================================
# íšŒì›ìš© ì±„íŒ… API (/analyze/chat)
# ==========================================
@app.post("/analyze/chat", response_model=ChatResponse)
async def analyze_chat(request: ChatRequest):
    """
    ìŠ¤í”„ë§ë¶€íŠ¸ì—ì„œ ì „ë‹¬ëœ í˜„ì¬ ëŒ€í™” ê¸°ë¡ + ê³¼ê±° 7ì¼ì¹˜ ë¡œê·¸ë¥¼ í™œìš©í•˜ì—¬
    LLM + RAG ê¸°ë°˜ ìƒë‹´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
    """
    try:
        # --- ì…ë ¥ê°’ í™•ì¸ ---
        if not request.message:
            raise HTTPException(status_code=400, detail="message í•„ë“œëŠ” ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # --- RAGì— ë„˜ê¸¸ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ---
        rag_context = {
            "diaryId": request.diaryId,
            "current_chat_history": [chat.dict() for chat in request.currentChatHistory] if request.currentChatHistory else [],
            "past_7days_history": request.past7DaysHistory
        }

        # --- LLM + RAGë¥¼ í†µí•´ ìƒë‹´ ë‹µë³€ ìƒì„± ---
        ai_response, debug_info = chat_with_rag_for_chat(
            user_text=request.message,
            context=rag_context
        )

        return JSONResponse(content={"message": ai_response})

    except HTTPException as http_ex:
        raise http_ex
    except Exception as e:
        print("DEBUG ERROR:", str(e))
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")

# ==========================================
# ë¹„íšŒì›ìš© ì±„íŒ… API (/analyze/chat/guest)
# ==========================================
class GuestChatRequest(BaseModel):
    message: Optional[str] = None
    currentChatHistory: Optional[List[ChatHistoryItem]] = None


@app.post("/analyze/chat/guest", response_model=ChatResponse)
async def analyze_chat_guest(request: GuestChatRequest):
    """
    ë¹„íšŒì›ì˜ ê²½ìš°, ê³¼ê±° ë¡œê·¸ ì—†ì´ í˜„ì¬ ëŒ€í™”ë§Œ ê¸°ë°˜ìœ¼ë¡œ AI ë‹µë³€ì„ ìƒì„±
    """
    try:
        if not request.message:
            raise HTTPException(status_code=400, detail="message í•„ë“œëŠ” ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        rag_context = {
            "current_chat_history": [chat.dict() for chat in request.currentChatHistory] if request.currentChatHistory else []
        }

        # RAGë¥¼ í˜¸ì¶œí•˜ë˜ ë¹„íšŒì› ëª¨ë“œë¡œ ì‹¤í–‰
        ai_response, debug_info = chat_with_rag_for_chat(
            user_text=request.message,
            context=rag_context
        )

        return JSONResponse(content={"message": ai_response})

    except HTTPException as http_ex:
        raise http_ex
    except Exception as e:
        print("DEBUG ERROR:", str(e))
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")


# --- ì„œë²„ ì§ì ‘ ì‹¤í–‰ì„ ìœ„í•œ ë¶€ë¶„ ---
if __name__ == "__main__":
    import multiprocessing
    import uvicorn

    # Windowsë‚˜ macOSì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì„¤ì •
    multiprocessing.set_start_method("spawn", force=True)
    
    # Uvicorn ì„œë²„ ì‹¤í–‰
    uvicorn.run(app, host="0.0.0.0", port=8000)