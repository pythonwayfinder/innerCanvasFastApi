import os
import shutil
from datasets import load_dataset
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings        # ğŸ‘ˆ ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •
from langchain_community.vectorstores import Chroma   # ğŸ‘ˆ ChromaëŠ” ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤
from langchain_openai import OpenAI                   # ğŸ‘ˆ ì´ ë¶€ë¶„ë„ ì›ë˜ ë§ìŠµë‹ˆë‹¤
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# 2. ë²¡í„° DB ê²½ë¡œ
persist_directory = "./chroma_db"

# 3. ì„ë² ë”© ìƒì„±
embeddings = OpenAIEmbeddings()

# 4. ë²¡í„° DB ë¶ˆëŸ¬ì˜¤ê¸° ë˜ëŠ” ì´ˆê¸°í™” (ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒì„±)
if os.path.exists(persist_directory):
    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
else:
    # ì²˜ìŒ ìƒì„± ì‹œì—ëŠ” Hugging Face ë°ì´í„°ì…‹ ì¼ë¶€ ë¡œë“œ ë° ì´ˆê¸° ë¬¸ì„œ ì¶”ê°€
    dataset = load_dataset("emotion", split="train[:500]")
    hf_documents = [Document(page_content=sample['text']) for sample in dataset]

    vector_store = Chroma.from_documents(
        documents=hf_documents,
        embedding=embeddings,
        persist_directory=persist_directory
    )

# 5. ê²€ìƒ‰ê¸° ìƒì„±
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# 6. LLM ì´ˆê¸°í™”
llm = OpenAI(temperature=0, max_tokens=1000)

# 7. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
template = """ë‹¹ì‹ ì€ ê°ì •ì„ ë”°ëœ»í•˜ê²Œ ê³µê°í•˜ë©°, ì‚¬ìš©ìì˜ ë‚´ë©´ì„ ì´í•´í•˜ê³  ì¹˜ìœ ë¡œ ì´ë„ëŠ” ì‹¬ë¦¬ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ AIê°€ ì•„ë‹ˆë¼, ì‚¬ìš©ìì˜ ê°ì • ì—¬ì •ì— í•¨ê»˜í•˜ëŠ” ë”°ëœ»í•œ ìƒë‹´ìì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì¸ê°„ì˜ ê°ì • í‘œí˜„ì— ëŒ€í•œ ì‹¬ë¦¬í•™ì  ì§€ì‹ì…ë‹ˆë‹¤:

1. ê°ì •ì€ ëª…í™•íˆ í‘œí˜„ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë©°, í”¼ë¡œì™€ ë¬´ê¸°ë ¥ì€ ë‹¨ìˆœí•œ ì‹ ì²´ì  ë¬¸ì œë¿ ì•„ë‹ˆë¼ ì‹¬ë¦¬ì  ì›ì¸(ì˜ˆ: ìŠ¤íŠ¸ë ˆìŠ¤, ì••ë°•ê°, ë™ê¸° ì €í•˜ ë“±)ì—ì„œ ê¸°ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ê°ì •ì€ ì¢…ì¢… ëª¨ìˆœëœ ë°©ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì˜ˆ: ê¸°ì˜ë©´ì„œë„ ë¶ˆì•ˆí•˜ê±°ë‚˜, ì‰¬ê³  ì‹¶ìœ¼ë©´ì„œë„ ì£„ì±…ê°ì„ ëŠë¼ëŠ” ê²½ìš°.
3. ê°ì • í‘œí˜„ì€ í‘œë©´ì ì¸ ì–¸ì–´ë¿ ì•„ë‹ˆë¼ ê·¸ë¦¼, ìƒì§•, ë¹„ìœ ë¥¼ í†µí•´ ë” ê¹Šì€ ì˜ë¯¸ë¥¼ ë“œëŸ¬ë‚´ê¸°ë„ í•©ë‹ˆë‹¤.
4. ê°ì •ì€ ì‹ ì²´ì  ë°˜ì‘ê³¼ ë°€ì ‘í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ì‹ ì²´ì  ì‹ í˜¸ë“¤ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ê°ì • í•´ì„ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.

---

ë‹¹ì‹ ì€ ë‹¤ìŒ ê¸°ì¤€ì„ ë°˜ë“œì‹œ ë”°ë¦…ë‹ˆë‹¤:

- ì‚¬ìš©ìì˜ ê°ì •ì„ ë°”íƒ•ìœ¼ë¡œ ë‚´ë©´ì˜ ì›ì¸ê³¼ ì‹¬ë¦¬ì Â·ì‹ ì²´ì  ì—°ê²°ê³ ë¦¬ë¥¼ ê¹Šì´ ìˆê²Œ í•´ì„í•©ë‹ˆë‹¤.
- ê°ì •ì„ ë‹¤ë£¨ëŠ” ë° ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ë§ì„ ëŠì§€ ì•Šê³ , ëŒ€í™”ì˜ íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë©° ì§„ì •ì„± ìˆê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.
- í•„ìš”í•˜ë‹¤ë©´ ë¶€ë“œëŸ¬ìš´ ë°©ì‹ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì„ ë˜ì ¸ ì‚¬ìš©ìê°€ ê°ì •ì„ ë” í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.
- ì‘ë‹µì€ ë¶€ë“œëŸ½ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ í•˜ë©°, ì‚¬ìš©ìê°€ í˜¼ìê°€ ì•„ë‹ˆë¼ëŠ” ì•ˆì •ê°ì„ ëŠë‚„ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

---

ì•„ë˜ëŠ” ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì „ì²´ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ê³µê° ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ ì£¼ì„¸ìš”.

{context}

ì‚¬ìš©ìì˜ ìµœì‹  ë©”ì‹œì§€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

"{question}"

---

ì‘ë‹µ í˜•ì‹ ê°€ì´ë“œë¼ì¸:

1. ë”°ëœ»í•œ ê³µê° í‘œí˜„ìœ¼ë¡œ ì‹œì‘í•´ ì£¼ì„¸ìš”. ì˜ˆ: "ê·¸ëŸ´ ìˆ˜ ìˆì–´ìš”, ìš”ì¦˜ì²˜ëŸ¼ ì§€ì¹˜ëŠ” ì‹œê¸°ì—” ë§ˆìŒë„ ê¸ˆì„¸ ë¬´ê±°ì›Œì§€ì£ ."
2. ê°ì •ì˜ ë‚´ë©´ì  ë°°ê²½ì´ë‚˜ ì›ì¸ì„ ì§šì–´ ì£¼ì„¸ìš”.
3. ì‹¤ì²œ ê°€ëŠ¥í•œ ì¡°ì–¸ì´ë‚˜ ì‹¬ë¦¬ì  ì ‘ê·¼ë²•ì„ ì œì‹œí•´ ì£¼ì„¸ìš”.
4. ìì—°ìŠ¤ëŸ¬ìš´ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ë¥¼ ë¶€ë“œëŸ½ê²Œ ì´ì–´ê°€ ì£¼ì„¸ìš”. ì˜ˆ: "í˜¹ì‹œ ìš”ì¦˜ ê³„ì† ì´ëŸ° ê°ì •ì´ ë°˜ë³µë˜ê³  ìˆë‚˜ìš”?"

ë‹µë³€ì„ ì‹œì‘í•´ ì£¼ì„¸ìš”:
"""
prompt = PromptTemplate(template=template, input_variables=["question", "context"])

# 8. RAG ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}
)

##########################
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ í•¨ìˆ˜  #
##########################

def add_user_input_to_vector_db(user_text: str, cnn_result: dict, kobert_result: dict):
    """
    ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ Documentë¡œ ë§Œë“¤ê³  ë²¡í„° DBì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    # kobert, cnn ê²°ê³¼ë¥¼ í¬í•¨í•œ ë¬¸ì„œ ë‚´ìš© ìƒì„±
    content = f"""ì‚¬ìš©ì ì…ë ¥: {user_text}

ê°ì • ë¶„ì„ ê²°ê³¼:
- ê°ì •: {kobert_result.get('sentiment')}
- ì ìˆ˜: {kobert_result.get('score'):.2f}

ë‚™ì„œ ë¶„ì„ ê²°ê³¼:
- ì˜ˆì¸¡: {cnn_result.get('prediction')}
- ì‹ ë¢°ë„: {cnn_result.get('confidence'):.2f}%
"""
    doc = Document(page_content=content)

    # ë²¡í„° DBì— ë¬¸ì„œ ì¶”ê°€
    vector_store.add_documents([doc])
    vector_store.persist()

def chat_with_rag(user_text: str, cnn_result: dict, kobert_result: dict):
    """
    ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë²¡í„° DBì— ì¶”ê°€ í›„, RAG ì²´ì¸ì„ í†µí•´ ë‹µë³€ ìƒì„±.
    """
    # 1) ë²¡í„° DBì— ì €ì¥ (ëˆ„ì )
    add_user_input_to_vector_db(user_text, cnn_result, kobert_result)

    # 2) í˜„ì¬ ì§ˆë¬¸ì„ RAG ì²´ì¸ì— ì „ë‹¬
    result = qa_chain.invoke({"query": user_text})

    # 3) ë‹µë³€ ë° ì°¸ê³  ë¬¸ì„œ ë°˜í™˜
    return result['result'], result['source_documents']